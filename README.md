# Задание 1: Сравнение CNN и полносвязных сетей (40 баллов)
## 1.1 Сравнение на MNIST (20 баллов)
Сравните производительность на MNIST:
- Полносвязная сеть (3-4 слоя)
- Простая CNN (2-3 conv слоя)
- CNN с Residual Block

Для каждого варианта:
- Обучите модель с одинаковыми гиперпараметрами\
batch_size: 128\
epochs: 10\
optimizer: Adam\
learning_rate: стандартный для Adam (обычно 0.001)\
loss_function: CrossEntropyLoss\
device: cuda (если доступна), иначе cpu\
Архитектуры моделей:\
FCNet: 4 слоя, размеры: 28×28 → 512 → 256 → 128 → 10\
SimpleCNN: 3 сверточных слоя с MaxPooling, полносвязные слои 128 → 10\
ResNetLike: 1 сверточный слой + 2 residual блока, adaptive avg pooling, fc 128 → 10
- Сравните точность на train и test множествах
```
Точность на train множестве:
FCNet: 0.9954
SimpleCNN: 0.9968
ResNetLike: 0.9983

Точность на test множестве:
FCNet: 0.9806
SimpleCNN: 0.9911
ResNetLike: 0.9905
```

- Измерьте время обучения и инференса
```
Время обучения (сек):
FCNet: 81.64
SimpleCNN: 280.84
ResNetLike: 591.11

Время инференса (сек):
FCNet: 0.0010
SimpleCNN: 0.0200
ResNetLike: 0.0300
```
- Визуализируйте кривые обучения

**Test accuracy**
![](plots/mnist_comparison/test_acc.png)

**Train accuracy**
![](plots/mnist_comparison/train_acc.png)

- Проанализируйте количество параметров
```
Сравнение числа параметров:
FCNet: 567434           # Самая большая модель по количеству параметров из-за крупных полносвязных слоев (512, 256, 128)
SimpleCNN: 241546       # Меньше параметров, так как сверточные слои используют локальные связи и параметр sharing
ResNetLike: 290058      # Немного больше параметров, чем SimpleCNN, за счет residual блоков и batch normalization; архитектура глубже и эффективнее
```

## 1.2 Сравнение на CIFAR-10 (20 баллов)
Сравните производительность на CIFAR-10:
- Полносвязная сеть (глубокая)
- CNN с Residual блоками
- CNN с регуляризацией и Residual блоками

Для каждого варианта:
- Обучите модель с одинаковыми гиперпараметрами\
batch_size: 128\
epochs: 10\
optimizer: Adam или SGD (momentum=0.9)\
learning_rate: 0.001 (Adam) или 0.1 (SGD, с понижением по эпохам)\
weight_decay: 1e-4 (для ResNet, регуляризация)\
dropout: 0.3–0.5 (для FCNet и регуляризованной CNN)\
augmentation: случайные сдвиги, горизонтальные отражения\
loss: CrossEntropyLoss

- Сравните точность и время обучения
```
Время обучения (сек):
DeepFCNet:              718.84
ResNetCIFAR:            1169.86
ResNetCIFARRegularized: 1294.81

Точность:
DeepFCNet:              Train acc: 0.4353 Test acc: 0.4781
ResNetCIFAR:            Train acc: 0.9105 Test acc: 0.8273
ResNetCIFARRegularized: Train acc: 0.7711 Test acc: 0.7934
```
- Проанализируйте переобучение\
ResNetCIFAR демонстрирует явные признаки переобучения: высокая точность и низкие потери на обучении, но заметно худшие результаты на тесте, особенно по мере увеличения эпох.\
Регуляризация в ResNetCIFARRegularized помогает уменьшить переобучение: разрыв между обучающими и тестовыми метриками становится меньше.\
DeepFCNet не переобучается, но и не достигает высоких результатов — вероятно, модель слишком простая для задачи.
- Визуализируйте confusion matrix

**DeepFCNet**
![](plots/cifar_comparison/conf_matrix_DeepFCNet.png)

**ResNetCIFAR**
![](plots/cifar_comparison/conf_matrix_ResNetCIFAR.png)

**ResNetCIFARRegularized**
![](plots/cifar_comparison/conf_matrix_ResNetCIFARRegularized.png)

- Исследуйте градиенты (gradient flow)\
Сеть демонстрирует типичное поведение градиентного потока для глубоких моделей: градиенты затухают по мере продвижения к более глубоким слоям, но не исчезают полностью. Это говорит о корректной работе механизма обратного распространения ошибки и отсутствии критических проблем с обучаемостью. Регуляризация влияет на распределение градиентов, делая их чуть более равномерными и предотвращая резкие всплески на отдельных слоях.

# Задание 2: Анализ архитектур CNN (30 баллов)
## 2.1 Влияние размера ядра свертки (15 баллов)
Исследуйте влияние размера ядра свертки:
- 3x3 ядра
- 5x5 ядра
- 7x7 ядра
- Комбинация разных размеров (1x1 + 3x3)

Для каждого варианта:
- Поддерживайте одинаковое количество параметров
```
Сравнение числа параметров:
CNN_3x3: 20042
CNN_5x5: 19922
CNN_7x7: 18782
CNN_Combo: 19274
```
- Сравните точность и время обучения
```
Сравнение точности:
CNN_3x3:    train acc: 0.5329   test acc: 0.5329
CNN_5x5:    train acc: 0.5562   test acc: 0.5503
CNN_7x7:    train acc: 0.5460   test acc: 0.5445
CNN_Combo:  train acc: 0.4554   test acc: 0.4378

Время обучения (сек):
CNN_3x3: 584.58
CNN_5x5: 584.88
CNN_7x7: 588.64
CNN_Combo: 580.48
```
- Проанализируйте рецептивные поля\
Размер рецептивного поля напрямую зависит от размера фильтра: чем больше фильтр, тем больше часть исходного изображения влияет на активацию нейрона.\
Малые рецептивные поля (3×3) — лучше для выделения локальных деталей и текстур.\
Большие рецептивные поля (5×5, 7×7) — эффективны для захвата глобальных особенностей и контуров.\
Комбинирование фильтров разных размеров позволяет сети быть чувствительной к признакам разного масштаба, что критически важно для сложных задач компьютерного зрения.

- Визуализируйте активации первого слоя

**3x3**
![](plots/architecture_analysis/act_CNN_3x3.png)

**5x5**
![](plots/architecture_analysis/act_CNN_5x5.png)

**7x7**
![](plots/architecture_analysis/act_CNN_7x7.png)

**1x1 + 3x3**
![](plots/architecture_analysis/act_CNN_Combo.png)

## 2.2 Влияние глубины CNN (15 баллов)
Исследуйте влияние глубины CNN:
- Неглубокая CNN (2 conv слоя)
- Средняя CNN (4 conv слоя)
- Глубокая CNN (6+ conv слоев)
- CNN с Residual связями

Для каждого варианта:
- Сравните точность и время обучения
```
Время обучения (сек):
ShallowCNN: 806.29
MediumCNN: 3333.88
DeepCNN: 6332.97
ResidualCNN: 11413.16

Сравнение точности:
ShallowCNN:     train acc: 0.4709   test acc: 0.4608
MediumCNN:      train acc: 0.6180   test acc: 0.6093
DeepCNN:        train acc: 0.6776   test acc: 0.6872
ResidualCNN:    train acc: 0.8504   test acc: 0.7883
```
- Проанализируйте vanishing/exploding gradients\
Затухание градиентов присутствует: чем глубже слой, тем меньше средний градиент, особенно в ResidualCNN, но residual-связи эффективно предотвращают полное исчезновение градиентов. В ShallowCNN затухание выражено сильнее между сверточными слоями, но не критично. Это может замедлять обучение начальных слоёв, особенно в очень глубоких сетях.\
Взрыва градиентов не обнаружено: все значения находятся в разумных пределах, что говорит о корректной инициализации весов и адекватной архитектуре.\
Стабильность: такая картина градиентного потока считается нормальной для современных сверточных сетей, особенно если используются методы вроде ReLU, BatchNorm и хорошая инициализация. Остаточные связи в ResidualCNN выполняют свою функцию, обеспечивая стабильное обучение даже в глубокой сети. В неглубокой ShallowCNN затухание незначительно и не мешает обучению.

- Исследуйте эффективность Residual связей\
Residual-связи радикально повышают эффективность обучения глубоких сверточных сетей: ускоряют рост точности, обеспечивают более быстрое снижение ошибки и предотвращают затухание градиентов. Это позволяет ResidualCNN превосходить классические архитектуры по всем ключевым метрикам на обучающей выборке.

- Визуализируйте feature maps

**ShallowCNN**
![](plots/architecture_analysis/fmap_ShallowCNN.png)

**MediumCNN**
![](plots/architecture_analysis/fmap_MediumCNN.png)

**DeepCNN**
![](plots/architecture_analysis/fmap_DeepCNN.png)

**ResidualCNN**
![](plots/architecture_analysis/fmap_ResidualCNN.png)

# Задание 3: Кастомные слои и эксперименты (30 баллов)
## 3.1 Реализация кастомных слоев (15 баллов)
Реализуйте кастомные слои:
- Кастомный сверточный слой с дополнительной логикой
- Attention механизм для CNN
- Кастомная функция активации
- Кастомный pooling слой

Для каждого слоя:
- Реализуйте forward и backward проходы\
Forward проходы реализованы, backward проходы автоматически вычисляет autograd.

- Добавьте параметры если необходимо\
В PyTorch параметры — это объекты типа nn.Parameter, которые автоматически добавляются к списку обучаемых параметров модели и обновляются оптимизатором.\
Так как у нас кастомные слои, которые имеют обучаемые параметры - мы их добавляем.

- Протестируйте на простых примерах
```
Результаты для 5 эпох:
Время обучения: 96.28 сек
train_acc: 0.8582
test_acc:  0.8876
```
- Сравните с стандартными аналогами
```
Время обучения моделей:
SimpleCNN: 131.26 сек
CNNWithCustomLayers: 200.80 сек
FCNet: 81.64 сек

Точность моделей
SimpleCNN:          train_acc: 0.9948   test_acc: 0.9880 (модель переобучилась)
CNNWithCustomLayers train_acc: 0.8872   test_acc: 0.8962
FCNet:              train_acc: 0.9954   test_acc: 0.9806 (модель переобучилась)
```

## 3.2 Эксперименты с Residual блоками (15 баллов)
Исследуйте различные варианты Residual блоков:
- Базовый Residual блок
- Bottleneck Residual блок
- Wide Residual блок

Для каждого варианта:
- Реализуйте блок с нуля\
Выполнено 👍
- Сравните производительность
```
Время обучения (сек):
BasicResNet:        589.55
BottleneckResNet:   583.22
WideResNet:         602.42
```
- Проанализируйте количество параметров
```
Сравнение числа параметров:
BasicResNet:        304810
BottleneckResNet:    33770
WideResNet:         886762
```
BottleneckResNet — самая компактная модель, подходит для задач, где критична скорость и экономия памяти.\
BasicResNet — компромисс между размером и качеством, классический вариант.\
WideResNet — самая "тяжёлая" и потенциально самая точная, но требует больше ресурсов.\
Честное сравнение: Bottleneck-блоки дают сильную экономию параметров, Wide-блоки — максимальное качество за счёт увеличения числа каналов.

- Исследуйте стабильность обучения\
Все три модели обучаются стабильно: loss и accuracy меняются плавно, нет взрывов/затухания, нет резких скачков.\
WideResNet — самая устойчивая и эффективная по качеству обучения из трёх.\
BottleneckResNet требует дополнительной настройки или глубины, чтобы достичь такой же стабильности и качества, как остальные.\
В целом, обучение ResNet-блоков в экспериментах проходит стабильно, без признаков нестабильности, связанных с градиентами или оптимизацией.

